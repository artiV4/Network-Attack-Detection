{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# import tensorflow_decision_forests as tfdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize and enumerate all attacks in dataset\n",
    "ATTACKS = ['DDoS', 'DoS', 'Mirai', 'Recon', 'Spoofing', 'Benign', 'Web', 'BruteForce']\n",
    "ATTACKS_ENUM = Enum('ATTACKS', ATTACKS, start=0)\n",
    "dict_7classes = {}\n",
    "dict_7classes['DDoS-RSTFINFlood'] = 'DDoS'\n",
    "dict_7classes['DDoS-PSHACK_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SYN_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-TCP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SynonymousIP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ACK_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-SlowLoris'] = 'DDoS'\n",
    "dict_7classes['DDoS-HTTP_Flood'] = 'DDoS'\n",
    "\n",
    "dict_7classes['DoS-UDP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-SYN_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-TCP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-HTTP_Flood'] = 'DoS'\n",
    "\n",
    "\n",
    "dict_7classes['Mirai-greeth_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-greip_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-udpplain'] = 'Mirai'\n",
    "\n",
    "dict_7classes['Recon-PingSweep'] = 'Recon'\n",
    "dict_7classes['Recon-OSScan'] = 'Recon'\n",
    "dict_7classes['Recon-PortScan'] = 'Recon'\n",
    "dict_7classes['VulnerabilityScan'] = 'Recon'\n",
    "dict_7classes['Recon-HostDiscovery'] = 'Recon'\n",
    "\n",
    "dict_7classes['DNS_Spoofing'] = 'Spoofing'\n",
    "dict_7classes['MITM-ArpSpoofing'] = 'Spoofing'\n",
    "\n",
    "dict_7classes['BenignTraffic'] = 'Benign'\n",
    "\n",
    "dict_7classes['BrowserHijacking'] = 'Web'\n",
    "dict_7classes['Backdoor_Malware'] = 'Web'\n",
    "dict_7classes['XSS'] = 'Web'\n",
    "dict_7classes['Uploading_Attack'] = 'Web'\n",
    "dict_7classes['SqlInjection'] = 'Web'\n",
    "dict_7classes['CommandInjection'] = 'Web'\n",
    "\n",
    "\n",
    "dict_7classes['DictionaryBruteForce'] = 'BruteForce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====Split Train / Test data======\n",
    "# Dataset link-> https://www.unb.ca/cic/datasets/iotdataset-2023.html\n",
    "#E. C. P. Neto, S. Dadkhah, R. Ferreira, A. Zohourian, R. Lu, A. A. Ghorbani. \"CICIoT2023: A real-time dataset and benchmark for large-scale attacks in IoT environment,\" Sensor (2023) – (submitted to Journal of Sensors).\n",
    "\n",
    "DATASET_DIRECTORY = 'dataset/'\n",
    "df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')] # all files\n",
    "#df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('1-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv')] # smaller subset for faster testing, 17 files =  10% of whole dataset\n",
    "#df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('11-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv')] # 2 files = 1%\n",
    "df_sets.sort()\n",
    "training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "test_sets = df_sets[int(len(df_sets)*.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====Extract Data=====\n",
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "       'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "       'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "] #columns 0-45\n",
    "Y_columns = 'label' #column 46\n",
    "\n",
    "all_columns = X_columns+[Y_columns]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "      'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "      'Rate', 'Srate', 'Drate', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "      'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "      'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "CATEGORICAL_FEATURE_NAMES = [\n",
    "      'fin_flag_number', 'syn_flag_number',\n",
    "      'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "      'ece_flag_number', 'cwr_flag_number','HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "      'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [04:04<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "#=====Feature Scaling======\n",
    "# columnsToScale = ['flow_duration', 'Header_Length', 'Duration', 'Rate', 'Srate', 'Drate', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'fin_count']\n",
    "# scale all\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "for train_set in tqdm(training_sets):\n",
    "    df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[X_columns]\n",
    "    x_train = scaler.fit(df)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose, epochs, batch_size = 0, 10, 512\n",
    "activationFunction='relu'\n",
    "\n",
    "# def getOtimizedSequentialModel():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(46, activation=activationFunction))\n",
    "#     model.add(Dense(30, activation=activationFunction))\n",
    "#     model.add(Dense(8, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return model\n",
    "\n",
    "def getManyLayersModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, activation=activationFunction))\n",
    "    model.add(Dense(30, activation=activationFunction))\n",
    "    model.add(Dense(20, activation=activationFunction))\n",
    "    model.add(Dense(12, activation=activationFunction))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "                    metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "                    )\n",
    "    return model\n",
    "\n",
    "# def getMoreLayersModel():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(46, activation=activationFunction))\n",
    "#     model.add(Dense(32, activation=activationFunction))\n",
    "#     model.add(Dense(22, activation=activationFunction))\n",
    "#     model.add(Dense(15, activation=activationFunction))\n",
    "#     model.add(Dense(10, activation=activationFunction))\n",
    "#     model.add(Dense(8, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return model\n",
    "\n",
    "def getManyLayersModel1024():\n",
    "    batch_size =  1024\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, activation=activationFunction))\n",
    "    model.add(Dense(30, activation=activationFunction))\n",
    "    model.add(Dense(20, activation=activationFunction))\n",
    "    model.add(Dense(12, activation=activationFunction))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "                    metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "                    )\n",
    "    return model\n",
    "\n",
    "def getManyLayersModel64():\n",
    "    batch_size =  64\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, activation=activationFunction))\n",
    "    model.add(Dense(30, activation=activationFunction))\n",
    "    model.add(Dense(20, activation=activationFunction))\n",
    "    model.add(Dense(12, activation=activationFunction))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "                    metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "                    )\n",
    "    return model\n",
    "\n",
    "# def getSeluModel():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(46, activation='selu'))\n",
    "#     model.add(Dense(30, activation='selu'))\n",
    "#     model.add(Dense(20, activation='selu'))\n",
    "#     model.add(Dense(12, activation='selu'))\n",
    "#     model.add(Dense(8, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return model\n",
    "\n",
    "ML_Models = [\n",
    "            getManyLayersModel(),\n",
    "            getManyLayersModel1024(),\n",
    "            getManyLayersModel64(),\n",
    "            # getThirdModel(),\n",
    "            # getFourthModel()\n",
    "\n",
    "]\n",
    "ML_Model_Names = [\n",
    "            'ManyLayers512',\n",
    "            'ManyLayers1024',\n",
    "            'ManyLayers64',\n",
    "            'Fourth'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ran on 3 models, with 135 training sets on date: 2024-05-31 22:54:25.001317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [1:16:27<00:00, 33.98s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last ran on {len(ML_Models)} models, with {len(training_sets)} training sets on date: {datetime.datetime.now()}\")\n",
    "for train_set in tqdm(training_sets):\n",
    "    df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "    x_train = scaler.transform(df[X_columns])\n",
    "    y_train = to_categorical([ATTACKS_ENUM[dict_7classes[k]].value for k in df[Y_columns]], num_classes=8)\n",
    "\n",
    "    for model in ML_Models:\n",
    "            model.fit(x=x_train, \n",
    "                        y=y_train, \n",
    "                        epochs=epochs, \n",
    "                        verbose=verbose,\n",
    "                        batch_size=batch_size)                   \n",
    "    del df\n",
    "    del x_train\n",
    "    del y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a seperate model to detect each attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose, epochs, batch_size = 1, 100, 512\n",
    "# activationFunction='relu'\n",
    "\n",
    "# def getSequentialModel():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(128, activation=activationFunction))\n",
    "#     model.add(Dense(64, activation=activationFunction))\n",
    "#     model.add(Dense(32, activation=activationFunction))\n",
    "#     model.add(Dense(16, activation=activationFunction))\n",
    "#     model.add(Dense(8, activation=activationFunction))\n",
    "#     model.add(Dense(4, activation=activationFunction))\n",
    "#     model.add(Dense(2, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return model\n",
    "\n",
    "# ML_Models = [\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel()\n",
    "\n",
    "# ]\n",
    "# ML_Model_Names = ATTACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Last ran on {len(ML_Models)} models, with {len(training_sets)} training sets on date: {datetime.datetime.now()}\")\n",
    "# for train_set in tqdm(training_sets):\n",
    "#     df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#     x_train = scaler.transform(df[X_columns])\n",
    "\n",
    "#     for i in range(len(ML_Models)-1):\n",
    "#             y_train = to_categorical([ATTACKS_ENUM[dict_7classes[k]].value == ATTACKS_ENUM[ATTACKS[i]].value for k in df[Y_columns]], num_classes=2)\n",
    "#             model = ML_Models[i]\n",
    "#             model.fit(x=x_train, \n",
    "#                         y=y_train, \n",
    "#                         epochs=epochs, \n",
    "#                         verbose=verbose,\n",
    "#                         batch_size=batch_size)   \n",
    "#             del y_train             \n",
    "#     del df\n",
    "#     del x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showResults8Models(test, pred, model_num):\n",
    "#     print(f\"===== {model_num} =====\")\n",
    "#     print(classification_report(test, pred, target_names=[\"Negative\", \"Positive\"]))\n",
    "#     accuracy = accuracy_score(test, pred)\n",
    "#     precision=precision_score(test, pred, average='weighted')\n",
    "#     f1Score=f1_score(test, pred, average='weighted') \n",
    "#     print(\"Accuracy  : {}\".format(accuracy))\n",
    "#     print(\"Precision : {}\".format(precision))\n",
    "#     print(\"f1Score : {}\".format(f1Score))\n",
    "#     cm=confusion_matrix(test, pred)\n",
    "#     print(cm) \n",
    "\n",
    "# print(f\"Last ran on {len(ML_Models)} models, with {len(test_sets)} testing sets on date: {datetime.datetime.now()}\")\n",
    "# for i in range(len(ML_Models)):\n",
    "#     model = ML_Models[i]\n",
    "#     y_test = []\n",
    "#     y_predict = []\n",
    "#     for test_set in tqdm(test_sets):\n",
    "#         df = pd.read_csv(DATASET_DIRECTORY + test_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#         x_test = scaler.transform(df[X_columns])\n",
    "#         for k in df[Y_columns]:\n",
    "#             y_test.append(ATTACKS_ENUM[dict_7classes[k]].value==ATTACKS[i])\n",
    "#         y_predict+= list(model.predict(x_test))\n",
    "\n",
    "#         del df\n",
    "#         del x_test\n",
    "\n",
    "#     myarr = np.array([ATTACKS_ENUM[dict_7classes[k]].value == ATTACKS_ENUM[ATTACKS[0]].value for k in ['DDoS-RSTFINFlood','DDoS-PSHACK_Flood','DDoS-SYN_Flood','DoS-SYN_Flood','DoS-TCP_Flood','Mirai-udpplain','Recon-OSScan','DNS_Spoofing','BrowserHijacking','Backdoor_Malware','DictionaryBruteForce']])\n",
    "#     print(myarr)\n",
    "#     print(to_categorical(myarr, num_classes=2))\n",
    "#     y_test=np.array(y_test)\n",
    "#     print(y_test[0:10])\n",
    "#     y_test = to_categorical(y_test, num_classes=2)\n",
    "#     print(y_test[0:10])\n",
    "#     print(\"=========\")\n",
    "#     for i in range(10):\n",
    "#         print(f\"{i}: {y_predict[i]} actual {y_test[i]}\")    \n",
    "\n",
    "#     test = np.argmax(y_test, axis=1)\n",
    "#     predict = np.argmax(y_predict, axis=1)\n",
    "#     showResults8Models(test, predict, i)\n",
    "\n",
    "#     del test\n",
    "#     del predict\n",
    "#     del y_test\n",
    "#     del y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the resulting trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveOutput(output, model_name):\n",
    "    savepath = f\"\\outputs\\{model_name}.txt\"\n",
    "    output=output\n",
    "    %store output >>\"\\outputs\\{}.txt\".format(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(test, pred,model_name):\n",
    "    output = ''\n",
    "    output += str(datetime.datetime.now())\n",
    "    output += f\"\\n===== {model_name} =====\\n\"\n",
    "    output+=classification_report(test, pred, target_names=ATTACKS)\n",
    "    accuracy = accuracy_score(test, pred)\n",
    "    precision=precision_score(test, pred, average='weighted')\n",
    "    f1Score=f1_score(test, pred, average='weighted') \n",
    "    output+=f\"\\nAccuracy  : {accuracy}\\n\"\n",
    "    output+=f\"Precision : {precision}\\n\"\n",
    "    output+=f\"f1Score : {f1Score}\\n\"\n",
    "    cm=confusion_matrix(test, pred)\n",
    "    output+=str(cm) \n",
    "    try:\n",
    "        %store output >>\"outputs\\output.txt\"\n",
    "    except Exception as e:\n",
    "        print(\"error saving to file\")\n",
    "        print(e)\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ran on 3 models, with 34 testing sets on date: 2024-06-01 00:10:52.766783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [05:32<00:00,  9.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'output' (str) to file 'outputs\\output.txt'.\n",
      "2024-06-01 00:16:29.263332\n",
      "===== ManyLayers512 =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DDoS       0.93      0.98      0.95   7526151\n",
      "         DoS       0.89      0.67      0.76   1792167\n",
      "       Mirai       1.00      0.99      1.00    583677\n",
      "       Recon       0.79      0.61      0.68     78630\n",
      "    Spoofing       0.77      0.72      0.74    107798\n",
      "      Benign       0.83      0.94      0.88    243322\n",
      "         Web       0.82      0.02      0.04      5433\n",
      "  BruteForce       1.00      0.14      0.25      2983\n",
      "\n",
      "    accuracy                           0.92  10340161\n",
      "   macro avg       0.88      0.63      0.66  10340161\n",
      "weighted avg       0.92      0.92      0.92  10340161\n",
      "\n",
      "Accuracy  : 0.9195814262466513\n",
      "Precision : 0.9178285410685464\n",
      "f1Score : 0.9151434016588018\n",
      "[[7370290  154588     500     626      87      60       0       0]\n",
      " [ 587465 1203804     322     537      23      16       0       0]\n",
      " [   2299     437  580670     202      48      17       4       0]\n",
      " [    951     732      60   47577    9832   19475       2       1]\n",
      " [    177       9      47    5071   77389   25090      15       0]\n",
      " [     46       4       9    4533   10377  228350       3       0]\n",
      " [      1       0       3    1098    2438    1785     108       0]\n",
      " [      0       0       0     829     640    1082       0     432]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [05:27<00:00,  9.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'output' (str) to file 'outputs\\output.txt'.\n",
      "2024-06-01 00:22:19.961822\n",
      "===== ManyLayers1024 =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DDoS       0.90      0.98      0.94   7526151\n",
      "         DoS       0.88      0.57      0.69   1792167\n",
      "       Mirai       1.00      0.99      1.00    583677\n",
      "       Recon       0.72      0.59      0.65     78630\n",
      "    Spoofing       0.78      0.67      0.73    107798\n",
      "      Benign       0.81      0.93      0.87    243322\n",
      "         Web       0.00      0.00      0.00      5433\n",
      "  BruteForce       0.86      0.01      0.03      2983\n",
      "\n",
      "    accuracy                           0.90  10340161\n",
      "   macro avg       0.75      0.59      0.61  10340161\n",
      "weighted avg       0.90      0.90      0.89  10340161\n",
      "\n",
      "Accuracy  : 0.9024167998931545\n",
      "Precision : 0.9005044563675687\n",
      "f1Score : 0.8939525263492153\n",
      "[[7392164  133363      95     378      78      73       0       0]\n",
      " [ 778335 1013310     122     337      50      13       0       0]\n",
      " [   2968     902  579567      79     155       5       0       1]\n",
      " [   2977     516       4   46216    7226   21688       0       3]\n",
      " [     33       8      29    7986   72741   26999       0       2]\n",
      " [     11       9      42    6298    9868  227093       0       1]\n",
      " [      3       0       0    1583    1986    1861       0       0]\n",
      " [      3       2       5    1098     651    1180       0      44]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [05:44<00:00, 10.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'output' (str) to file 'outputs\\output.txt'.\n",
      "2024-06-01 00:28:27.325831\n",
      "===== ManyLayers64 =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DDoS       0.96      0.96      0.96   7526151\n",
      "         DoS       0.83      0.82      0.83   1792167\n",
      "       Mirai       0.99      1.00      0.99    583677\n",
      "       Recon       0.76      0.59      0.66     78630\n",
      "    Spoofing       0.72      0.74      0.73    107798\n",
      "      Benign       0.84      0.91      0.87    243322\n",
      "         Web       0.00      0.00      0.00      5433\n",
      "  BruteForce       1.00      0.14      0.25      2983\n",
      "\n",
      "    accuracy                           0.93  10340161\n",
      "   macro avg       0.76      0.64      0.66  10340161\n",
      "weighted avg       0.93      0.93      0.93  10340161\n",
      "\n",
      "Accuracy  : 0.9307869577659381\n",
      "Precision : 0.9298765895608769\n",
      "f1Score : 0.9301129130356994\n",
      "[[7229593  293654    2199     371     311      23       0       0]\n",
      " [ 323627 1466802    1336     182     214       6       0       0]\n",
      " [   1734     576  581239      59      64       5       0       0]\n",
      " [   2547     889     447   46068   11891   16786       0       2]\n",
      " [     46      29      76    5320   79500   22827       0       0]\n",
      " [     21      25      10    6984   15428  220854       0       0]\n",
      " [      0       2       1    1258    2630    1542       0       0]\n",
      " [      1       9       0     751     901     890       0     431]]\n"
     ]
    }
   ],
   "source": [
    "def testModel(model, model_name):\n",
    "    y_test = []\n",
    "    y_predict = []\n",
    "    for test_set in tqdm(test_sets):\n",
    "        df = pd.read_csv(DATASET_DIRECTORY + test_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "        x_test = scaler.transform(df[X_columns])\n",
    "        for k in df[Y_columns]:\n",
    "            y_test.append(ATTACKS_ENUM[dict_7classes[k]].value)\n",
    "        y_predict+= list(model.predict(x_test, verbose=0))\n",
    "\n",
    "        del df\n",
    "        del x_test\n",
    "\n",
    "    y_test=np.array(y_test)\n",
    "    y_test = to_categorical(y_test, num_classes=8)\n",
    "    test = np.argmax(y_test, axis=1)\n",
    "    predict = np.argmax(y_predict, axis=1)\n",
    "    showResults(test, predict, model_name)\n",
    "\n",
    "    del test\n",
    "    del predict\n",
    "    del y_test\n",
    "    del y_predict\n",
    "\n",
    "print(f\"Last ran on {len(ML_Models)} models, with {len(test_sets)} testing sets on date: {datetime.datetime.now()}\")\n",
    "for i in range(len(ML_Models)):\n",
    "    testModel(ML_Models[i],ML_Model_Names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====For debug ===\n",
    "# test_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')] \n",
    "# testModel(load_model(\"SavedModels\\\\ManyLayers.keras\"), \"ManyLayers from file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ML_Models)):\n",
    "    ML_Models[i].save(f\"SavedModels\\\\{ML_Model_Names[i]}.keras\",overwrite=True)\n",
    "    #del ML_Models[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Maximum number of decision trees. The effective number of trained trees can be smaller if early stopping is enabled.\n",
    "# NUM_TREES = 250\n",
    "# # Minimum number of examples in a node.\n",
    "# MIN_EXAMPLES = 6\n",
    "# # Maximum depth of the tree. max_depth=1 means that all trees will be roots.\n",
    "# MAX_DEPTH = 5\n",
    "# # Ratio of the dataset (sampling without replacement) used to train individual trees for the random sampling method.\n",
    "# SUBSAMPLE = 0.65\n",
    "# # Control the sampling of the datasets used to train individual trees.\n",
    "# SAMPLING_METHOD = \"RANDOM\"\n",
    "# # Ratio of the training dataset used to monitor the training. Require to be >0 if early stopping is enabled.\n",
    "# VALIDATION_RATIO = 0.1\n",
    "\n",
    "# epochs_df = 100\n",
    "# batch_size_df = 512\n",
    "# verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def specify_feature_usages():\n",
    "#     feature_usages = []\n",
    "\n",
    "#     for feature_name in NUMERIC_FEATURE_NAMES:\n",
    "#         feature_usage = tfdf.keras.FeatureUsage(\n",
    "#             name=feature_name, semantic=tfdf.keras.FeatureSemantic.NUMERICAL\n",
    "#         )\n",
    "#         feature_usages.append(feature_usage)\n",
    "\n",
    "#     for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "#         feature_usage = tfdf.keras.FeatureUsage(\n",
    "#             name=feature_name, semantic=tfdf.keras.FeatureSemantic.CATEGORICAL\n",
    "#         )\n",
    "#         feature_usages.append(feature_usage)\n",
    "\n",
    "#     return feature_usages\n",
    "\n",
    "\n",
    "# def create_gbt_model():\n",
    "#     # See all the model parameters in https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/GradientBoostedTreesModel\n",
    "#     gbt_model = tfdf.keras.GradientBoostedTreesModel(\n",
    "#         features=specify_feature_usages(),\n",
    "#         exclude_non_specified_features=True,\n",
    "#         num_trees=NUM_TREES,\n",
    "#         max_depth=MAX_DEPTH,\n",
    "#         min_examples=MIN_EXAMPLES,\n",
    "#         subsample=SUBSAMPLE,\n",
    "#         validation_ratio=VALIDATION_RATIO,\n",
    "#         task=tfdf.keras.Task.CLASSIFICATION,\n",
    "#     )\n",
    "\n",
    "#     gbt_model.compile(metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
    "#     return gbt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tfdf.keras.RandomForestModel()\n",
    "# for train_set in tqdm(training_sets):\n",
    "#     tf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(train_set,label=\"species\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #======== Cross Validation ===========\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# skf.get_n_splits(xTrain, yTrain)\n",
    "# foldNum=0\n",
    "# for train_index, val_index in skf.split(xTrain, yTrain):\n",
    "#     foldNum+=1\n",
    "#     print(\"Results for fold\",foldNum)\n",
    "#     X_train, X_val = X[train_index], X[val_index]\n",
    "#     Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "    \n",
    "#     # one hot encode\n",
    "#     Y_train = to_categorical(Y_train)\n",
    "#     Y_val = to_categorical(Y_val)\n",
    "    \n",
    "#     history = model.fit(X_train, Y_train, \n",
    "#                         validation_data = (X_val, Y_val), \n",
    "#                         epochs=epochs, \n",
    "#                         batch_size=batch_size)  \n",
    "#     yPredict = model.predict(X_val)\n",
    "\n",
    "#     #Converting one hot encoded test label to label    \n",
    "#     pred = np.argmax(yPredict, axis=1)\n",
    "#     val = np.argmax(Y_val, axis=1)\n",
    "    \n",
    "#     showResults(val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============Test Phase============\n",
    "# yPred = model.predict(xTest)\n",
    "# yTest = to_categorical(yTest)\n",
    "# pred = np.argmax(yPred, axis=1)\n",
    "# test = np.argmax(yTest, axis=1)\n",
    "# showResults(test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
