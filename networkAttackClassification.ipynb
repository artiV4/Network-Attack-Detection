{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier # Unsupervised\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import tensorflow_decision_forests as tfdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize and enumerate all attacks in dataset\n",
    "ATTACKS = ['DDoS', 'DoS', 'Mirai', 'Recon', 'Spoofing', 'Benign', 'Web', 'BruteForce']\n",
    "ATTACKS_ENUM = Enum('ATTACKS', ATTACKS, start=0)\n",
    "dict_7classes = {}\n",
    "dict_7classes['DDoS-RSTFINFlood'] = 'DDoS'\n",
    "dict_7classes['DDoS-PSHACK_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SYN_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-TCP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SynonymousIP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ACK_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-SlowLoris'] = 'DDoS'\n",
    "dict_7classes['DDoS-HTTP_Flood'] = 'DDoS'\n",
    "\n",
    "dict_7classes['DoS-UDP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-SYN_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-TCP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-HTTP_Flood'] = 'DoS'\n",
    "\n",
    "\n",
    "dict_7classes['Mirai-greeth_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-greip_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-udpplain'] = 'Mirai'\n",
    "\n",
    "dict_7classes['Recon-PingSweep'] = 'Recon'\n",
    "dict_7classes['Recon-OSScan'] = 'Recon'\n",
    "dict_7classes['Recon-PortScan'] = 'Recon'\n",
    "dict_7classes['VulnerabilityScan'] = 'Recon'\n",
    "dict_7classes['Recon-HostDiscovery'] = 'Recon'\n",
    "\n",
    "dict_7classes['DNS_Spoofing'] = 'Spoofing'\n",
    "dict_7classes['MITM-ArpSpoofing'] = 'Spoofing'\n",
    "\n",
    "dict_7classes['BenignTraffic'] = 'Benign'\n",
    "\n",
    "dict_7classes['BrowserHijacking'] = 'Web'\n",
    "dict_7classes['Backdoor_Malware'] = 'Web'\n",
    "dict_7classes['XSS'] = 'Web'\n",
    "dict_7classes['Uploading_Attack'] = 'Web'\n",
    "dict_7classes['SqlInjection'] = 'Web'\n",
    "dict_7classes['CommandInjection'] = 'Web'\n",
    "\n",
    "\n",
    "dict_7classes['DictionaryBruteForce'] = 'BruteForce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Generation of CSV data =====\n",
    "# Adapted from same source as Dataset\n",
    "run_this = False\n",
    "if(run_this):\n",
    "    from pcap2csv import Generating_dataset #Generating_dataset#, Supporting_functions, Communication_features, Connectivity_features, Dynamic_features, Feature_extraction, Layered_features\n",
    "    import os\n",
    "    PCAP_DIRECTORY = 'pcap/'\n",
    "    pcap_files = [k for k in os.listdir(PCAP_DIRECTORY) if k.endswith('.pcap')] \n",
    "    Generating_dataset.make_csv(pcap_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====Split Train / Test data======\n",
    "# Dataset link-> https://www.unb.ca/cic/datasets/iotdataset-2023.html\n",
    "#E. C. P. Neto, S. Dadkhah, R. Ferreira, A. Zohourian, R. Lu, A. A. Ghorbani. \"CICIoT2023: A real-time dataset and benchmark for large-scale attacks in IoT environment,\" Sensor (2023) – (submitted to Journal of Sensors).\n",
    "\n",
    "DATASET_DIRECTORY = 'dataset/'\n",
    "df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')] # all files\n",
    "#df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('1-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv')] # smaller subset for faster testing, 17 files =  10% of whole dataset\n",
    "#df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('11-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv')] # 2 files = 1%\n",
    "df_sets.sort()\n",
    "training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "test_sets = df_sets[int(len(df_sets)*.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====Extract Data=====\n",
    "# X_columns = [\n",
    "#     'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "#        'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "#        'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "#        'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "#        'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "#     'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "#        'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "#        'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "#        'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "# ] #columns 0-45\n",
    "# Y_columns = 'label' #column 46\n",
    "\n",
    "# === For testing of \"live\" data, do not train on data which is the summation of packet flow ===\n",
    "X_columns = [\n",
    "    'Header_Length', 'Protocol Type', 'Duration',\n",
    "        'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC',\n",
    "] #columns 0-45\n",
    "Y_columns = 'label' #column 46\n",
    "\n",
    "all_columns = X_columns+[Y_columns]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "      'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "      'Rate', 'Srate', 'Drate', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "      'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "      'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "CATEGORICAL_FEATURE_NAMES = [\n",
    "      'fin_flag_number', 'syn_flag_number',\n",
    "      'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "      'ece_flag_number', 'cwr_flag_number','HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "      'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [02:18<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "#=====Feature Scaling======\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler(feature_range=(-1,1)) # for SVC\n",
    "for train_set in tqdm(training_sets):\n",
    "    df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[X_columns]\n",
    "    scaler.fit(df)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, model, name, type, callbacks=None):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.callbacks = callbacks\n",
    "        #self.batch_size = batch_size\n",
    "\n",
    "TYPES = {}\n",
    "TYPES['SK_LR'] = 1\n",
    "TYPES['SK_RF'] = 2\n",
    "TYPES['SK_SVC'] = 3\n",
    "TYPES['TF'] = 4\n",
    "TYPES['TF_RNN'] = 5\n",
    "TYPES['TF_CNN'] = 6\n",
    "\n",
    "verbose, epochs, batch_size = 0, 10, 64\n",
    "activationFunction='relu'\n",
    "\n",
    "numFeatures=len(X_columns)\n",
    "numClasses=len(ATTACKS)\n",
    "\n",
    "# def getOtimizedSequentialModel():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(46, activation=activationFunction))\n",
    "#     model.add(Dense(30, activation=activationFunction))\n",
    "#     model.add(Dense(8, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return model\n",
    "\n",
    "def getANN():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, activation=activationFunction))\n",
    "    model.add(Dense(30, activation=activationFunction))\n",
    "    model.add(Dense(20, activation=activationFunction))\n",
    "    model.add(Dense(12, activation=activationFunction))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "                    metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "                    )\n",
    "    return Model(model, \"ANN\", TYPES['TF'])\n",
    "\n",
    "def getANN2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=numFeatures, activation=activationFunction))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation=activationFunction))\n",
    "    model.add(Dense(32, activation=activationFunction))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "                    metrics=[ keras.metrics.BinaryAccuracy()]\n",
    "                    )\n",
    "    return Model(model, \"ANN2\", TYPES['TF'])\n",
    "\n",
    "def getANN3():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=numFeatures, activation=activationFunction))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation=activationFunction))\n",
    "    model.add(Dense(32, activation=activationFunction))\n",
    "    model.add(Dense(16, activation=activationFunction))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "                    metrics=[ keras.metrics.BinaryAccuracy()]\n",
    "                    )\n",
    "    return Model(model, \"ANN3\", TYPES['TF'])\n",
    "\n",
    "def getRNN():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return Model(model, \"RNN\", TYPES['TF_RNN'])\n",
    "\n",
    "def getCNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=1, activation='relu', input_shape=(numFeatures, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(32, kernel_size=1, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return Model(model, 'CNN', TYPES['TF_CNN'])\n",
    "\n",
    "def getCNN2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(numFeatures, 1), kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    return Model(model, 'CNN', TYPES['TF_CNN'], [early_stopping])\n",
    "\n",
    "def getRFModel():\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        criterion = 'gini',\n",
    "        max_depth=None,\n",
    "        )\n",
    "    return Model(rf, \"RF\", TYPES['SK_RF'])\n",
    "\n",
    "def getSimpleRFModel():\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=1,\n",
    "        criterion = 'gini',\n",
    "        max_depth=None,\n",
    "        )\n",
    "    return Model(rf, \"RF\", TYPES['SK_RF'])\n",
    "\n",
    "def getDTModel():\n",
    "    rf = DecisionTreeClassifier(\n",
    "        criterion = 'gini',\n",
    "        max_depth=None,\n",
    "        )\n",
    "    return Model(rf, \"DT\", TYPES['SK_RF'])\n",
    "\n",
    "def getLRModel():\n",
    "    lr = LogisticRegression()\n",
    "    return Model(lr, \"LR\", TYPES['SK_LR'])\n",
    "\n",
    "def getSVCModel():\n",
    "    model = SVC()\n",
    "    return Model(model, \"SVCovr\", TYPES['SK_SVC'])\n",
    "\n",
    "def getovoSVCModel():\n",
    "    model = SVC(decision_function_shape='ovo', verbose=False, cache_size=1000)\n",
    "    return Model(model, \"SVCovo\", TYPES['SK_SVC'])\n",
    "\n",
    "def getLinSVCModel():\n",
    "    model = LinearSVC(tol = 1e-5)\n",
    "    return Model(model, \"Lin-SVC\", TYPES['SK_LR'])\n",
    "\n",
    "def getSGDCModel():\n",
    "    return Model(SGDClassifier(loss='log_loss', penalty='l1', alpha=1e-8, max_iter=3000, tol=1e-5, random_state=11), \"SGDC2\", TYPES['SK_LR'])\n",
    "\n",
    "def getKNNCModel():\n",
    "    model = KNeighborsClassifier()\n",
    "    return Model(model, \"KNNC\", TYPES['SK_LR'])\n",
    "\n",
    "def getRadNNCModel():\n",
    "    model = RadiusNeighborsClassifier()\n",
    "    return Model(model, \"RadNNC\", TYPES['SK_LR'])\n",
    "\n",
    "def getNCentModel():\n",
    "    model = NearestCentroid()\n",
    "    return Model(model, \"NCent\", TYPES['SK_LR'])\n",
    "\n",
    "def getRidgeModel():\n",
    "    model = RidgeClassifier(solver='saga')\n",
    "    return Model(model, \"Ridge\", TYPES['SK_RF'])\n",
    "\n",
    "# === Test bottom models later ===\n",
    "\n",
    "# def getBRBMModel():\n",
    "#     model = BernoulliRBM()\n",
    "#     return Model(model, \"Bernoulli Restricted Bolzman Machine\", TYPES['SK_LR'])\n",
    "\n",
    "def getBNBModel():\n",
    "    model = BernoulliNB()\n",
    "    return Model(model, \"Bernoulli Naive Bayes\", TYPES['SK_LR'])\n",
    "\n",
    "def getCNBModel():\n",
    "    model = CategoricalNB()\n",
    "    return Model(model, \"Categocical Naive Bayes\", TYPES['SK_LR'])\n",
    "\n",
    "def getGNBModel():\n",
    "    model = GaussianNB()\n",
    "    return Model(model, \"Gausian Naive Bayes\", TYPES['SK_LR'])\n",
    "\n",
    "def getMLPCModel():\n",
    "    model = MLPClassifier() #MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)  \n",
    "    return Model(model, \"MLPC\", TYPES['SK_LR'])\n",
    "\n",
    "def getNUSVCModel():\n",
    "    model = NuSVC()\n",
    "    return Model(model, \"NuSVC\", TYPES['SK_LR'])\n",
    "\n",
    "ML_Models = [\n",
    "            # getSVCModel(), # bad - infinite training\n",
    "            # === batch 1\n",
    "            # getLinSVCModel(), # good\n",
    "            # getSGDCModel(), #good\n",
    "            # getSGDC_LogLossModel(),\n",
    "            # getSGDC_HuberModel(),\n",
    "            # getKNNCModel(), #good results - long prediction time\n",
    "            # getNCentModel(), # good\n",
    "            # getRidgeModel(), # good\n",
    "\n",
    "            # === batch 2\n",
    "            #getRadNNCModel(), #bad - infinite testing?\n",
    "\n",
    "            # === batch 3\n",
    "\n",
    "            # getBRBMModel(), # - no predict \n",
    "            # getBNBModel(),\n",
    "            # getCNBModel(),\n",
    "\n",
    "            # getGNBModel(), # repeat experimentation\n",
    "            \n",
    "            # getNUSVCModel(), # bad - error\n",
    "            # getMLPCModel(), # unsupervised\n",
    "            # getDTModel(),\n",
    "            # getSVCModel()\n",
    "\n",
    "            # getSGDCModel(), #92% # was with (-1, 1) scaler?\n",
    "            # getRNN(), # 98%\n",
    "            # getCNN(), #98%\n",
    "            # getANN(),\n",
    "            # getANN2(),\n",
    "            # getANN3(),\n",
    "            getRFModel(),\n",
    "\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ran on 1 models, with 135 training sets on date: 2024-06-20 15:16:06.419016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [2:08:41<00:00, 57.19s/it]  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Last ran on {len(ML_Models)} models, with {len(training_sets)} training sets on date: {datetime.datetime.now()}\")\n",
    "for train_set in tqdm(training_sets):\n",
    "    df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "    x_train = scaler.transform(df[X_columns])\n",
    "    y_train = [ATTACKS_ENUM[dict_7classes[k]].value for k in df[Y_columns]]\n",
    "    y_train_Cat = to_categorical(y_train, num_classes=numClasses)\n",
    "\n",
    "    for i in range(len(ML_Models)):\n",
    "            # print(f\"model{i}\")\n",
    "            model = ML_Models[i]\n",
    "            if model.type==TYPES[\"SK_LR\"]:\n",
    "                model.model.fit(x_train, y_train)  \n",
    "            \n",
    "            if model.type==TYPES[\"SK_SVC\"]:\n",
    "                length = len(x_train) \n",
    "                print(length)\n",
    "                slicesize = length//8\n",
    "                for j in range(0, length, slicesize):\n",
    "                    curr_x_train = x_train[j:j+slicesize]\n",
    "                    curr_y_train = y_train[j:j+slicesize]\n",
    "                    print(f\"training on {j}\")\n",
    "                    model.model.fit(curr_x_train, curr_y_train)\n",
    "                del curr_x_train\n",
    "                # model.model.fit(x_train, y_train) \n",
    "\n",
    "\n",
    "            elif model.type==TYPES[\"SK_RF\"]:\n",
    "                model.model.fit(x_train, y_train_Cat)  \n",
    "\n",
    "            elif model.type == TYPES[\"TF\"]:\n",
    "                model.model.fit(x=x_train, \n",
    "                            y=y_train_Cat, \n",
    "                            epochs=epochs, \n",
    "                            verbose=verbose,\n",
    "                            batch_size=batch_size) \n",
    "\n",
    "            elif model.type == TYPES['TF_RNN']: \n",
    "                curr_x_train= np.reshape(x_train, (x_train.shape[0], 1, numFeatures))\n",
    "                model.model.fit(x=curr_x_train, \n",
    "                            y=y_train_Cat, \n",
    "                            epochs=epochs, \n",
    "                            verbose=verbose,\n",
    "                            batch_size=batch_size) \n",
    "                del curr_x_train\n",
    "\n",
    "            elif model.type == TYPES['TF_CNN']: \n",
    "                curr_x_train= np.reshape(x_train, (x_train.shape[0], numFeatures, 1))\n",
    "                model.model.fit(x=curr_x_train, \n",
    "                            y=y_train_Cat, \n",
    "                            epochs=epochs, \n",
    "                            verbose=verbose,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=model.callbacks\n",
    "                            )    \n",
    "                del curr_x_train         \n",
    "    del df\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del y_train_Cat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the resulting trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(test, pred,model_name):\n",
    "    output = ''\n",
    "    output += str(datetime.datetime.now())\n",
    "    output += f\"\\n===== {model_name} =====\\n\"\n",
    "    output+=classification_report(test, pred, target_names=ATTACKS)\n",
    "    accuracy = accuracy_score(test, pred)\n",
    "    precision=precision_score(test, pred, average='weighted')\n",
    "    f1Score=f1_score(test, pred, average='weighted') \n",
    "    output+=f\"\\nAccuracy  : {accuracy}\\n\"\n",
    "    output+=f\"Precision : {precision}\\n\"\n",
    "    output+=f\"f1Score : {f1Score}\\n\"\n",
    "    cm=confusion_matrix(test, pred)\n",
    "    output+=str(cm) \n",
    "    \n",
    "    joblib.dump(output, f\"outputs/{model_name}.txt\") \n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model):\n",
    "    y_test = []\n",
    "    y_predict = []\n",
    "    for test_set in tqdm(test_sets):\n",
    "        df = pd.read_csv(DATASET_DIRECTORY + test_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "        x_test = scaler.transform(df[X_columns])\n",
    "        for k in df[Y_columns]:\n",
    "            y_test.append(ATTACKS_ENUM[dict_7classes[k]].value)\n",
    "        if model.type == TYPES['TF'] or model.type==TYPES['TF_CNN']:\n",
    "            y_predict+= list(model.model.predict(x_test, verbose=0))\n",
    "        elif model.type== TYPES[\"TF_RNN\"]:\n",
    "            x_test= np.reshape(x_test, (x_test.shape[0], 1, numFeatures))\n",
    "            y_predict+= list(model.model.predict(x_test, verbose=0))\n",
    "        elif model.type==TYPES[\"SK_SVC\"]:\n",
    "            length = len(x_test) \n",
    "            print(length)\n",
    "            slicesize = length//8\n",
    "            for j in range(0, length, slicesize):\n",
    "                curr_x_test = x_test[j:j+slicesize]\n",
    "                print(f\"testing on {j}\")\n",
    "                y_predict+= list(model.model.predict(curr_x_test))\n",
    "        else:\n",
    "            y_predict+= list(model.model.predict(x_test))\n",
    "\n",
    "        del df\n",
    "        del x_test\n",
    "\n",
    "    y_test=np.array(y_test)\n",
    "    if model.type == TYPES['TF'] or model.type ==TYPES[\"SK_RF\"] or model.type==TYPES[\"TF_RNN\"] or model.type==TYPES['TF_CNN']:\n",
    "        y_test = to_categorical(y_test, num_classes=8)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "        y_predict = np.argmax(y_predict, axis=1)\n",
    "    showResults(y_test, y_predict, model.name)\n",
    "\n",
    "\n",
    "    del y_test\n",
    "    del y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ML_Models)):\n",
    "    model = ML_Models[i]\n",
    "    if model.type==TYPES[\"TF\"]:\n",
    "        model.model.save(f\"SavedModels\\\\{model.name}.keras\",overwrite=True)\n",
    "    else:\n",
    "        joblib.dump(model.model, f\"SavedModels/{model.name}.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ran on 1 models, with 34 testing sets on date: 2024-06-20 17:24:48.124042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [05:13<00:00,  9.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-20 17:30:06.499763\n",
      "===== RF =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DDoS       0.90      0.94      0.92   7526151\n",
      "         DoS       0.70      0.60      0.65   1792167\n",
      "       Mirai       0.98      0.97      0.98    583677\n",
      "       Recon       0.71      0.39      0.50     78630\n",
      "    Spoofing       0.64      0.42      0.51    107798\n",
      "      Benign       0.74      0.75      0.75    243322\n",
      "         Web       0.27      0.03      0.05      5433\n",
      "  BruteForce       0.43      0.07      0.13      2983\n",
      "\n",
      "    accuracy                           0.87  10340161\n",
      "   macro avg       0.67      0.52      0.56  10340161\n",
      "weighted avg       0.86      0.87      0.86  10340161\n",
      "\n",
      "Accuracy  : 0.8670591299303754\n",
      "Precision : 0.8601930425119871\n",
      "f1Score : 0.8618904348618682\n",
      "[[7064710  446971    7485    1473    1046    4456       4       6]\n",
      " [ 713121 1073583    1998    1171     720    1569       5       0]\n",
      " [   9983    3151  567701     217    1816     808       1       0]\n",
      " [  24285    1395      32   30617    3197   18975      82      47]\n",
      " [  21414    1268    1881    2667   45440   34955     128      45]\n",
      " [  34757     660      83    6049   18275  183102     196     200]\n",
      " [   2695      81       4     493     646    1356     155       3]\n",
      " [   1363      37       1     243     188     926       2     223]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last ran on {len(ML_Models)} models, with {len(test_sets)} testing sets on date: {datetime.datetime.now()}\")\n",
    "# temp_df_sets=[k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "# test_sets=temp_df_sets[int(len(temp_df_sets)*.8):]\n",
    "for i in range(len(ML_Models)):\n",
    "    testModel(ML_Models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {\n",
    "#     # 'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "#     'loss': ['log_loss'],\n",
    "#     'penalty': ['l2', 'l1'],\n",
    "#     # 'penalty': ['l2'],\n",
    "#     'alpha': [1e-10, 1e-9, 1e-8, 1e-7],\n",
    "#     'max_iter': [1000],\n",
    "#     'tol': [1e-5]\n",
    "# }\n",
    "# grid_search = GridSearchCV(SGDClassifier(), param_grid, scoring='accuracy', n_jobs=-1)\n",
    "# print(f\"Last ran on grid, with {len(training_sets)} training sets on date: {datetime.datetime.now()}\")\n",
    "# for train_set in tqdm(training_sets):\n",
    "#     df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#     x_train = scaler.transform(df[X_columns])\n",
    "#     y_train = [ATTACKS_ENUM[dict_7classes[k]].value for k in df[Y_columns]]\n",
    "\n",
    "#     grid_search.fit(x_train, y_train)                   \n",
    "#     del df\n",
    "#     del x_train\n",
    "#     del y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# testModel(Model(best_model, \"SGDC FromGrid\", TYPES[\"SK_LR\"]))\n",
    "# # print(\"Best Model Accuracy:\", best_model.score(x_test, y_test))\n",
    "# # print(\"Best Model Classification Report:\\n\", classification_report(y_test, best_model.predict(X_test)))\n",
    "# # print(\"Best Model Confusion Matrix:\\n\", confusion_matrix(y_test, best_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ML_Models[i].model\n",
    "# for test_set in test_sets:\n",
    "#     df = pd.read_csv(DATASET_DIRECTORY + test_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#     x_test = scaler.transform(df[X_columns])\n",
    "#     y_test = [ATTACKS_ENUM[dict_7classes[k]].value for k in df[Y_columns]]\n",
    "#     print(\"predicting\")\n",
    "#     y_predict = model.predict(x_test)\n",
    "#     print(\"done\")\n",
    "#     y_test=np.array(y_test)\n",
    "\n",
    "#     showResults(y_test, y_predict, \"TEst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====For debug ===\n",
    "# test_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')] \n",
    "#testModel(Model(load_model(\"SavedModels\\\\BestANN.keras\"),\"BestANN\", TYPES['TF']))\n",
    "# testModel(Model(joblib.load(\"SavedModels/BestRF.pkl\"),\"BestRF\", TYPES['SK_RF']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = getSimpleRFModel()\n",
    "# num_sample = 10\n",
    "# for train_set in tqdm(training_sets):\n",
    "#     df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#     df = df[0:num_sample]\n",
    "#     x_train = scaler.transform(df[X_columns])\n",
    "#     y_train = [ATTACKS_ENUM[dict_7classes[k]].value for k in df[Y_columns]]\n",
    "#     y_train_Cat = to_categorical(y_train, num_classes=8)\n",
    "\n",
    "    \n",
    "#     if model.type==TYPES[\"SK_LR\"]:\n",
    "#         model.model.fit(x_train, y_train)  \n",
    "    \n",
    "#     elif model.type==TYPES[\"SK_RF\"]:\n",
    "#         model.model.fit(x_train, y_train_Cat)  \n",
    "\n",
    "#     elif model.type == TYPES[\"TF\"]:\n",
    "#         model.model.fit(x=x_train, \n",
    "#                     y=y_train_Cat, \n",
    "#                     epochs=epochs, \n",
    "#                     verbose=verbose,\n",
    "#                     batch_size=batch_size)                   \n",
    "#     del df\n",
    "#     del x_train\n",
    "#     del y_train\n",
    "#     del y_train_Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tree Visualisation\n",
    "# from sklearn.tree import export_graphviz\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn import tree\n",
    "# #rf = joblib.load(\"SavedModels/BestRF.pkl\")\n",
    "# # for tree_in_rf in rf:\n",
    "# #     fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "# #     # export_graphviz(tree,\n",
    "# #     #             feature_names=X_columns,\n",
    "# #     #             class_names=ATTACKS,\n",
    "# #     #             filled=True,\n",
    "# #     #             rounded=True)\n",
    "# #     tree.plot_tree(rf.estimators_[0],\n",
    "# #                 feature_names=X_columns,\n",
    "# #                 class_names=ATTACKS,\n",
    "# #                filled = True)\n",
    "# #     fig.savefig(f'rf_{i}.png')\n",
    "# #     i+=1\n",
    "# # for tree_in_rf in rf[:1]:\n",
    "# #     export_graphviz(tree_in_rf,\n",
    "# #                 feature_names=X_columns,\n",
    "# #                 class_names=ATTACKS,\n",
    "# #                 filled=True,\n",
    "# #                 rounded=True,\n",
    "# #                 out_file=\"tree.png\")\n",
    "# #     os.system(\"-Tpng tree.dot -o tree.png\")\n",
    "# # fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "# # tree.plot_tree(model.model.estimators_[0])#,\n",
    "# #             # feature_names=X_columns,\n",
    "# #             # class_names=ATTACKS,\n",
    "# #             # filled = True)\n",
    "# # fig.savefig(f'rf_{num_sample}_samples.png')\n",
    "\n",
    "\n",
    "\n",
    "# # == Feature importance for RF and DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(ML_Models)):\n",
    "#     model = ML_Models[i]\n",
    "#     if model.type==TYPES[\"TF\"]:\n",
    "#         model.model.save(f\"SavedModels\\\\{model.name}.keras\",overwrite=True)\n",
    "#     elif model.type==TYPES[\"SK_RF\"] or model.type==TYPES[\"SK_LR\"]:\n",
    "#         joblib.dump(model.model, f\"SavedModels/{model.name}.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a seperate model to detect each attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose, epochs, batch_size = 1, 100, 512\n",
    "# activationFunction='relu'\n",
    "\n",
    "# def getSequentialModel():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(128, activation=activationFunction))\n",
    "#     model.add(Dense(64, activation=activationFunction))\n",
    "#     model.add(Dense(32, activation=activationFunction))\n",
    "#     model.add(Dense(16, activation=activationFunction))\n",
    "#     model.add(Dense(8, activation=activationFunction))\n",
    "#     model.add(Dense(4, activation=activationFunction))\n",
    "#     model.add(Dense(2, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return model\n",
    "\n",
    "# ML_Models = [\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel()\n",
    "\n",
    "# ]\n",
    "# ML_Model_Names = ATTACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Last ran on {len(ML_Models)} models, with {len(training_sets)} training sets on date: {datetime.datetime.now()}\")\n",
    "# for train_set in tqdm(training_sets):\n",
    "#     df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#     x_train = scaler.transform(df[X_columns])\n",
    "\n",
    "#     for i in range(len(ML_Models)-1):\n",
    "#             y_train = to_categorical([ATTACKS_ENUM[dict_7classes[k]].value == ATTACKS_ENUM[ATTACKS[i]].value for k in df[Y_columns]], num_classes=2)\n",
    "#             model = ML_Models[i]\n",
    "#             model.fit(x=x_train, \n",
    "#                         y=y_train, \n",
    "#                         epochs=epochs, \n",
    "#                         verbose=verbose,\n",
    "#                         batch_size=batch_size)   \n",
    "#             del y_train             \n",
    "#     del df\n",
    "#     del x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showResults8Models(test, pred, model_num):\n",
    "#     print(f\"===== {model_num} =====\")\n",
    "#     print(classification_report(test, pred, target_names=[\"Negative\", \"Positive\"]))\n",
    "#     accuracy = accuracy_score(test, pred)\n",
    "#     precision=precision_score(test, pred, average='weighted')\n",
    "#     f1Score=f1_score(test, pred, average='weighted') \n",
    "#     print(\"Accuracy  : {}\".format(accuracy))\n",
    "#     print(\"Precision : {}\".format(precision))\n",
    "#     print(\"f1Score : {}\".format(f1Score))\n",
    "#     cm=confusion_matrix(test, pred)\n",
    "#     print(cm) \n",
    "\n",
    "# print(f\"Last ran on {len(ML_Models)} models, with {len(test_sets)} testing sets on date: {datetime.datetime.now()}\")\n",
    "# for i in range(len(ML_Models)):\n",
    "#     model = ML_Models[i]\n",
    "#     y_test = []\n",
    "#     y_predict = []\n",
    "#     for test_set in tqdm(test_sets):\n",
    "#         df = pd.read_csv(DATASET_DIRECTORY + test_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#         x_test = scaler.transform(df[X_columns])\n",
    "#         for k in df[Y_columns]:\n",
    "#             y_test.append(ATTACKS_ENUM[dict_7classes[k]].value==ATTACKS[i])\n",
    "#         y_predict+= list(model.predict(x_test))\n",
    "\n",
    "#         del df\n",
    "#         del x_test\n",
    "\n",
    "#     myarr = np.array([ATTACKS_ENUM[dict_7classes[k]].value == ATTACKS_ENUM[ATTACKS[0]].value for k in ['DDoS-RSTFINFlood','DDoS-PSHACK_Flood','DDoS-SYN_Flood','DoS-SYN_Flood','DoS-TCP_Flood','Mirai-udpplain','Recon-OSScan','DNS_Spoofing','BrowserHijacking','Backdoor_Malware','DictionaryBruteForce']])\n",
    "#     print(myarr)\n",
    "#     print(to_categorical(myarr, num_classes=2))\n",
    "#     y_test=np.array(y_test)\n",
    "#     print(y_test[0:10])\n",
    "#     y_test = to_categorical(y_test, num_classes=2)\n",
    "#     print(y_test[0:10])\n",
    "#     print(\"=========\")\n",
    "#     for i in range(10):\n",
    "#         print(f\"{i}: {y_predict[i]} actual {y_test[i]}\")    \n",
    "\n",
    "#     test = np.argmax(y_test, axis=1)\n",
    "#     predict = np.argmax(y_predict, axis=1)\n",
    "#     showResults8Models(test, predict, i)\n",
    "\n",
    "#     del test\n",
    "#     del predict\n",
    "#     del y_test\n",
    "#     del y_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
