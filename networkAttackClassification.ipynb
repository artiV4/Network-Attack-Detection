{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier # Unsupervised\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import tensorflow_decision_forests as tfdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize and enumerate all attacks in dataset\n",
    "ATTACKS = ['DDoS', 'DoS', 'Mirai', 'Recon', 'Spoofing', 'Benign', 'Web', 'BruteForce']\n",
    "ATTACKS_ENUM = Enum('ATTACKS', ATTACKS, start=0)\n",
    "dict_7classes = {}\n",
    "dict_7classes['DDoS-RSTFINFlood'] = 'DDoS'\n",
    "dict_7classes['DDoS-PSHACK_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SYN_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-TCP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-SynonymousIP_Flood'] = 'DDoS'\n",
    "dict_7classes['DDoS-ACK_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-UDP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-ICMP_Fragmentation'] = 'DDoS'\n",
    "dict_7classes['DDoS-SlowLoris'] = 'DDoS'\n",
    "dict_7classes['DDoS-HTTP_Flood'] = 'DDoS'\n",
    "\n",
    "dict_7classes['DoS-UDP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-SYN_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-TCP_Flood'] = 'DoS'\n",
    "dict_7classes['DoS-HTTP_Flood'] = 'DoS'\n",
    "\n",
    "\n",
    "dict_7classes['Mirai-greeth_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-greip_flood'] = 'Mirai'\n",
    "dict_7classes['Mirai-udpplain'] = 'Mirai'\n",
    "\n",
    "dict_7classes['Recon-PingSweep'] = 'Recon'\n",
    "dict_7classes['Recon-OSScan'] = 'Recon'\n",
    "dict_7classes['Recon-PortScan'] = 'Recon'\n",
    "dict_7classes['VulnerabilityScan'] = 'Recon'\n",
    "dict_7classes['Recon-HostDiscovery'] = 'Recon'\n",
    "\n",
    "dict_7classes['DNS_Spoofing'] = 'Spoofing'\n",
    "dict_7classes['MITM-ArpSpoofing'] = 'Spoofing'\n",
    "\n",
    "dict_7classes['BenignTraffic'] = 'Benign'\n",
    "\n",
    "dict_7classes['BrowserHijacking'] = 'Web'\n",
    "dict_7classes['Backdoor_Malware'] = 'Web'\n",
    "dict_7classes['XSS'] = 'Web'\n",
    "dict_7classes['Uploading_Attack'] = 'Web'\n",
    "dict_7classes['SqlInjection'] = 'Web'\n",
    "dict_7classes['CommandInjection'] = 'Web'\n",
    "\n",
    "\n",
    "dict_7classes['DictionaryBruteForce'] = 'BruteForce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Generation of CSV data =====\n",
    "# Adapted from same source as Dataset\n",
    "run_this = False\n",
    "if(run_this):\n",
    "    from pcap2csv import Generating_dataset #Generating_dataset#, Supporting_functions, Communication_features, Connectivity_features, Dynamic_features, Feature_extraction, Layered_features\n",
    "    import os\n",
    "    PCAP_DIRECTORY = 'pcap/'\n",
    "    pcap_files = [k for k in os.listdir(PCAP_DIRECTORY) if k.endswith('.pcap')] \n",
    "    Generating_dataset.make_csv(pcap_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====Split Train / Test data======\n",
    "# Dataset link-> https://www.unb.ca/cic/datasets/iotdataset-2023.html\n",
    "#E. C. P. Neto, S. Dadkhah, R. Ferreira, A. Zohourian, R. Lu, A. A. Ghorbani. \"CICIoT2023: A real-time dataset and benchmark for large-scale attacks in IoT environment,\" Sensor (2023) – (submitted to Journal of Sensors).\n",
    "\n",
    "DATASET_DIRECTORY = 'dataset/'\n",
    "df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')] # all files\n",
    "#df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('1-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv')] # smaller subset for faster testing, 17 files =  10% of whole dataset\n",
    "#df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('11-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv')] # 2 files = 1%\n",
    "df_sets.sort()\n",
    "training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "test_sets = df_sets[int(len(df_sets)*.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====Extract Data=====\n",
    "\n",
    "# === PerFlow ===\n",
    "# X_columns = [\n",
    "#     'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "#        'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "#        'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "#        'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "#        'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "#     'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "#        'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "#        'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "#        'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "# ] #columns 0-45\n",
    "# Y_columns = 'label' #column 46\n",
    "\n",
    "# === PerPacket ===\n",
    "X_columns = [\n",
    "    'Header_Length', 'Protocol Type', 'Duration',\n",
    "        'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC',\n",
    "] #columns 0-45\n",
    "Y_columns = 'label' #column 46\n",
    "\n",
    "all_columns = X_columns+[Y_columns]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "      'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "      'Rate', 'Srate', 'Drate', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "      'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "      'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "CATEGORICAL_FEATURE_NAMES = [\n",
    "      'fin_flag_number', 'syn_flag_number',\n",
    "      'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "      'ece_flag_number', 'cwr_flag_number','HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "      'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [03:08<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "#=====Feature Scaling======\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler(feature_range=(-1,1)) # for SVC\n",
    "for train_set in tqdm(training_sets):\n",
    "    df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[X_columns]\n",
    "    scaler.fit(df)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, model, name, type, callbacks=None):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "        self.callbacks = callbacks\n",
    "        #self.batch_size = batch_size\n",
    "\n",
    "TYPES = {}\n",
    "TYPES['SK_LR'] = 1\n",
    "TYPES['SK_RF'] = 2\n",
    "TYPES['SK_SVC'] = 3\n",
    "TYPES['TF'] = 4\n",
    "TYPES['TF_RNN'] = 5\n",
    "TYPES['TF_CNN'] = 6\n",
    "\n",
    "verbose, epochs, batch_size = 0, 10, 64\n",
    "activationFunction='relu'\n",
    "\n",
    "numFeatures=len(X_columns)\n",
    "numClasses=len(ATTACKS)\n",
    "\n",
    "# def getOtimizedSequentialModel():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(46, activation=activationFunction))\n",
    "#     model.add(Dense(30, activation=activationFunction))\n",
    "#     model.add(Dense(8, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return model\n",
    "\n",
    "# def getANN():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(46, activation=activationFunction))\n",
    "#     model.add(Dense(30, activation=activationFunction))\n",
    "#     model.add(Dense(20, activation=activationFunction))\n",
    "#     model.add(Dense(12, activation=activationFunction))\n",
    "#     model.add(Dense(numClasses, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return Model(model, \"ANN\", TYPES['TF'])\n",
    "\n",
    "def getANN():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=numFeatures, activation=activationFunction))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation=activationFunction))\n",
    "    model.add(Dense(32, activation=activationFunction))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "                    metrics=[ keras.metrics.BinaryAccuracy()]\n",
    "                    )\n",
    "    return Model(model, \"ANN2\", TYPES['TF'])\n",
    "\n",
    "# def getANN3():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(128, input_dim=numFeatures, activation=activationFunction))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(64, activation=activationFunction))\n",
    "#     model.add(Dense(32, activation=activationFunction))\n",
    "#     model.add(Dense(16, activation=activationFunction))\n",
    "#     model.add(Dense(numClasses, activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy()]\n",
    "#                     )\n",
    "#     return Model(model, \"ANN3\", TYPES['TF'])\n",
    "\n",
    "def getRNN():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return Model(model, \"RNN\", TYPES['TF_RNN'])\n",
    "\n",
    "def getCNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=1, activation='relu', input_shape=(numFeatures, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(32, kernel_size=1, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(numClasses, activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return Model(model, 'CNN', TYPES['TF_CNN'])\n",
    "\n",
    "# def getCNN2():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(numFeatures, 1), kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(numClasses, activation='softmax'))\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "#     return Model(model, 'CNN', TYPES['TF_CNN'], [early_stopping])\n",
    "\n",
    "def getRFModel():\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        criterion = 'gini',\n",
    "        max_depth=None,\n",
    "        )\n",
    "    return Model(rf, \"RF\", TYPES['SK_RF'])\n",
    "\n",
    "def getDTModel():\n",
    "    rf = DecisionTreeClassifier(\n",
    "        criterion = 'gini',\n",
    "        max_depth=None,\n",
    "        )\n",
    "    return Model(rf, \"DT\", TYPES['SK_RF'])\n",
    "\n",
    "def getLRModel():\n",
    "    lr = LogisticRegression()\n",
    "    return Model(lr, \"LR\", TYPES['SK_LR'])\n",
    "\n",
    "def getSVCModel():\n",
    "    model = SVC()\n",
    "    return Model(model, \"SVCovr\", TYPES['SK_SVC'])\n",
    "\n",
    "def getovoSVCModel():\n",
    "    model = SVC(decision_function_shape='ovo', verbose=False, cache_size=1000)\n",
    "    return Model(model, \"SVCovo\", TYPES['SK_SVC'])\n",
    "\n",
    "def getLinSVCModel():\n",
    "    model = LinearSVC(tol = 1e-5)\n",
    "    return Model(model, \"Lin-SVC\", TYPES['SK_LR'])\n",
    "\n",
    "def getSGDCModel():\n",
    "    return Model(SGDClassifier(loss='log_loss', penalty='l1', alpha=1e-8, max_iter=3000, tol=1e-5, random_state=11), \"SGDC\", TYPES['SK_LR'])\n",
    "\n",
    "def getKNNCModel():\n",
    "    model = KNeighborsClassifier()\n",
    "    return Model(model, \"KNNC\", TYPES['SK_LR'])\n",
    "\n",
    "def getRadNNCModel():\n",
    "    model = RadiusNeighborsClassifier()\n",
    "    return Model(model, \"RadNNC\", TYPES['SK_LR'])\n",
    "\n",
    "def getNCentModel():\n",
    "    model = NearestCentroid()\n",
    "    return Model(model, \"NCent\", TYPES['SK_LR'])\n",
    "\n",
    "def getRidgeModel():\n",
    "    model = RidgeClassifier(solver='saga')\n",
    "    return Model(model, \"Ridge\", TYPES['SK_RF'])\n",
    "\n",
    "def getBNBModel():\n",
    "    model = BernoulliNB()\n",
    "    return Model(model, \"Bernoulli Naive Bayes\", TYPES['SK_LR'])\n",
    "\n",
    "def getCNBModel():\n",
    "    model = CategoricalNB()\n",
    "    return Model(model, \"Categorical Naive Bayes\", TYPES['SK_LR'])\n",
    "\n",
    "def getGNBModel():\n",
    "    model = GaussianNB()\n",
    "    return Model(model, \"Gausian Naive Bayes\", TYPES['SK_LR'])\n",
    "\n",
    "def getMLPCModel():\n",
    "    model = MLPClassifier() #MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)  \n",
    "    return Model(model, \"MLPC\", TYPES['SK_LR'])\n",
    "\n",
    "def getNUSVCModel():\n",
    "    model = NuSVC()\n",
    "    return Model(model, \"NuSVC\", TYPES['SK_LR'])\n",
    "\n",
    "mod1 = getKNNCModel()\n",
    "mod1.name=\"KNNC2\"\n",
    "mod2 = getKNNCModel()\n",
    "mod2.name=\"KNNC3\"\n",
    "mod3 = getCNN()\n",
    "mod3.name=\"CNN3\"\n",
    "mod4 = getRNN()\n",
    "mod4.name=\"RNN\"\n",
    "mod5 = getSGDCModel()\n",
    "mod5.name=\"SGDC\"\n",
    "ML_Models = [\n",
    "            mod1,\n",
    "            mod2,\n",
    "            # mod3,\n",
    "            # mod4,\n",
    "            # mod5\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ran on 2 models, with 135 training sets on date: 2024-06-27 09:21:33.652441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [03:22<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last ran on {len(ML_Models)} models, with {len(training_sets)} training sets on date: {datetime.datetime.now()}\")\n",
    "for train_set in tqdm(training_sets):\n",
    "    df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "    x_train = scaler.transform(df[X_columns])\n",
    "    y_train = [ATTACKS_ENUM[dict_7classes[k]].value for k in df[Y_columns]]\n",
    "    y_train_Cat = to_categorical(y_train, num_classes=numClasses)\n",
    "\n",
    "    for i in range(len(ML_Models)):\n",
    "            # print(f\"model{i}\")\n",
    "            model = ML_Models[i]\n",
    "            if model.type==TYPES[\"SK_LR\"]:\n",
    "                model.model.fit(x_train, y_train)  \n",
    "            \n",
    "            if model.type==TYPES[\"SK_SVC\"]:\n",
    "                length = len(x_train) \n",
    "                print(length)\n",
    "                slicesize = length//8\n",
    "                for j in range(0, length, slicesize):\n",
    "                    curr_x_train = x_train[j:j+slicesize]\n",
    "                    curr_y_train = y_train[j:j+slicesize]\n",
    "                    print(f\"training on {j}\")\n",
    "                    model.model.fit(curr_x_train, curr_y_train)\n",
    "                del curr_x_train\n",
    "\n",
    "            elif model.type==TYPES[\"SK_RF\"]:\n",
    "                model.model.fit(x_train, y_train_Cat)  \n",
    "\n",
    "            elif model.type == TYPES[\"TF\"]:\n",
    "                model.model.fit(x=x_train, \n",
    "                            y=y_train_Cat, \n",
    "                            epochs=epochs, \n",
    "                            verbose=verbose,\n",
    "                            batch_size=batch_size) \n",
    "\n",
    "            elif model.type == TYPES['TF_RNN']: \n",
    "                curr_x_train= np.reshape(x_train, (x_train.shape[0], 1, numFeatures))\n",
    "                model.model.fit(x=curr_x_train, \n",
    "                            y=y_train_Cat, \n",
    "                            epochs=epochs, \n",
    "                            verbose=verbose,\n",
    "                            batch_size=batch_size) \n",
    "                del curr_x_train\n",
    "\n",
    "            elif model.type == TYPES['TF_CNN']: \n",
    "                curr_x_train= np.reshape(x_train, (x_train.shape[0], numFeatures, 1))\n",
    "                model.model.fit(x=curr_x_train, \n",
    "                            y=y_train_Cat, \n",
    "                            epochs=epochs, \n",
    "                            verbose=verbose,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=model.callbacks\n",
    "                            )    \n",
    "                del curr_x_train         \n",
    "    del df\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del y_train_Cat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the resulting trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(test, pred,model_name):\n",
    "    output = ''\n",
    "    output += str(datetime.datetime.now())\n",
    "    output += f\"\\n===== {model_name} =====\\n\"\n",
    "    output+=classification_report(test, pred, target_names=ATTACKS)\n",
    "    accuracy = accuracy_score(test, pred)\n",
    "    precision=precision_score(test, pred, average='weighted')\n",
    "    f1Score=f1_score(test, pred, average='weighted') \n",
    "    output+=f\"\\nAccuracy  : {accuracy}\\n\"\n",
    "    output+=f\"Precision : {precision}\\n\"\n",
    "    output+=f\"f1Score : {f1Score}\\n\"\n",
    "    cm=confusion_matrix(test, pred)\n",
    "    output+=str(cm) \n",
    "    \n",
    "    joblib.dump(output, f\"outputs/{model_name}.txt\", protocol=1) \n",
    "\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model):\n",
    "    y_test = []\n",
    "    y_predict = []\n",
    "    for test_set in tqdm(test_sets):\n",
    "        df = pd.read_csv(DATASET_DIRECTORY + test_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "        x_test = scaler.transform(df[X_columns])\n",
    "        for k in df[Y_columns]:\n",
    "            y_test.append(ATTACKS_ENUM[dict_7classes[k]].value)\n",
    "        if model.type == TYPES['TF'] or model.type==TYPES['TF_CNN']:\n",
    "            y_predict+= list(model.model.predict(x_test, verbose=0))\n",
    "        elif model.type== TYPES[\"TF_RNN\"]:\n",
    "            x_test= np.reshape(x_test, (x_test.shape[0], 1, numFeatures))\n",
    "            y_predict+= list(model.model.predict(x_test, verbose=0))\n",
    "        elif model.type==TYPES[\"SK_SVC\"]:\n",
    "            length = len(x_test) \n",
    "            print(length)\n",
    "            slicesize = length//8\n",
    "            for j in range(0, length, slicesize):\n",
    "                curr_x_test = x_test[j:j+slicesize]\n",
    "                print(f\"testing on {j}\")\n",
    "                y_predict+= list(model.model.predict(curr_x_test))\n",
    "        else:\n",
    "            y_predict+= list(model.model.predict(x_test))\n",
    "\n",
    "        del df\n",
    "        del x_test\n",
    "\n",
    "    y_test=np.array(y_test)\n",
    "    if model.type == TYPES['TF'] or model.type ==TYPES[\"SK_RF\"] or model.type==TYPES[\"TF_RNN\"] or model.type==TYPES['TF_CNN']:\n",
    "        y_test = to_categorical(y_test, num_classes=8)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "        y_predict = np.argmax(y_predict, axis=1)\n",
    "    showResults(y_test, y_predict, model.name)\n",
    "\n",
    "\n",
    "    del y_test\n",
    "    del y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for timing, remove file i/o times\n",
    "def onlyPredict(model):\n",
    "    y_test = []\n",
    "    y_predict = []\n",
    "    for test_set in tqdm(test_sets):\n",
    "        df = pd.read_csv(DATASET_DIRECTORY + test_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "        x_test = scaler.transform(df[X_columns])\n",
    "        for k in df[Y_columns]:\n",
    "            y_test.append(ATTACKS_ENUM[dict_7classes[k]].value)\n",
    "        if model.type == TYPES['TF'] or model.type==TYPES['TF_CNN']:\n",
    "            y_predict+= list(model.model.predict(x_test, verbose=0))\n",
    "        elif model.type== TYPES[\"TF_RNN\"]:\n",
    "            x_test= np.reshape(x_test, (x_test.shape[0], 1, numFeatures))\n",
    "            y_predict+= list(model.model.predict(x_test, verbose=0))\n",
    "        elif model.type==TYPES[\"SK_SVC\"]:\n",
    "            length = len(x_test) \n",
    "            print(length)\n",
    "            slicesize = length//8\n",
    "            for j in range(0, length, slicesize):\n",
    "                curr_x_test = x_test[j:j+slicesize]\n",
    "                print(f\"testing on {j}\")\n",
    "                y_predict+= list(model.model.predict(curr_x_test))\n",
    "        else:\n",
    "            y_predict+= list(model.model.predict(x_test))\n",
    "\n",
    "        del df\n",
    "        del x_test\n",
    "\n",
    "    y_test=np.array(y_test)\n",
    "    if model.type == TYPES['TF'] or model.type ==TYPES[\"SK_RF\"] or model.type==TYPES[\"TF_RNN\"] or model.type==TYPES['TF_CNN']:\n",
    "        y_test = to_categorical(y_test, num_classes=8)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "        y_predict = np.argmax(y_predict, axis=1)\n",
    "\n",
    "\n",
    "    del y_test\n",
    "    del y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ML_Models)):\n",
    "    model = ML_Models[i]\n",
    "    if model.type==TYPES[\"TF\"]:# or model.type==TYPES['TF_CNN'] or model.type==TYPES['TF_RNN']:\n",
    "        model.model.save(f\"SavedModels\\\\{model.name}.keras\",overwrite=True)\n",
    "    else:\n",
    "        joblib.dump(model.model, f\"SavedModels/{model.name}.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ran on 2 models, with 34 testing sets on date: 2024-06-27 09:24:56.681639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [46:12<00:00, 81.55s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-27 10:11:10.036660\n",
      "===== KNNC2 =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DDoS       0.90      0.94      0.92   7526151\n",
      "         DoS       0.70      0.58      0.63   1792167\n",
      "       Mirai       0.97      0.98      0.97    583677\n",
      "       Recon       0.51      0.47      0.49     78630\n",
      "    Spoofing       0.50      0.39      0.44    107798\n",
      "      Benign       0.69      0.76      0.73    243322\n",
      "         Web       0.03      0.01      0.01      5433\n",
      "  BruteForce       0.82      0.15      0.25      2983\n",
      "\n",
      "    accuracy                           0.87  10340161\n",
      "   macro avg       0.64      0.53      0.55  10340161\n",
      "weighted avg       0.86      0.87      0.86  10340161\n",
      "\n",
      "Accuracy  : 0.8653204722827816\n",
      "Precision : 0.8582076734243121\n",
      "f1Score : 0.8602734859584316\n",
      "[[7077469  426824    6101    4435    2552    8716      48       6]\n",
      " [ 747601 1035530    1433    2981    1221    3384      13       4]\n",
      " [   5309    3953  569135     786    2663    1813      11       7]\n",
      " [   5838    4823     314   36798    7344   23284     209      20]\n",
      " [   4270    3765    6186    8043   42198   43054     255      27]\n",
      " [   7227    4228     832   17063   27645  185953     344      30]\n",
      " [    479     412      35    1345    1168    1962      28       4]\n",
      " [    287     142      10     587     423    1077      15     442]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [48:55<00:00, 86.34s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-27 11:00:34.430701\n",
      "===== KNNC3 =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DDoS       0.90      0.94      0.92   7526151\n",
      "         DoS       0.70      0.58      0.63   1792167\n",
      "       Mirai       0.97      0.98      0.97    583677\n",
      "       Recon       0.51      0.47      0.49     78630\n",
      "    Spoofing       0.50      0.39      0.44    107798\n",
      "      Benign       0.69      0.76      0.73    243322\n",
      "         Web       0.03      0.01      0.01      5433\n",
      "  BruteForce       0.82      0.15      0.25      2983\n",
      "\n",
      "    accuracy                           0.87  10340161\n",
      "   macro avg       0.64      0.53      0.55  10340161\n",
      "weighted avg       0.86      0.87      0.86  10340161\n",
      "\n",
      "Accuracy  : 0.8653204722827816\n",
      "Precision : 0.8582076734243121\n",
      "f1Score : 0.8602734859584316\n",
      "[[7077469  426824    6101    4435    2552    8716      48       6]\n",
      " [ 747601 1035530    1433    2981    1221    3384      13       4]\n",
      " [   5309    3953  569135     786    2663    1813      11       7]\n",
      " [   5838    4823     314   36798    7344   23284     209      20]\n",
      " [   4270    3765    6186    8043   42198   43054     255      27]\n",
      " [   7227    4228     832   17063   27645  185953     344      30]\n",
      " [    479     412      35    1345    1168    1962      28       4]\n",
      " [    287     142      10     587     423    1077      15     442]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last ran on {len(ML_Models)} models, with {len(test_sets)} testing sets on date: {datetime.datetime.now()}\")\n",
    "# temp_df_sets=[k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "# test_sets=temp_df_sets[int(len(temp_df_sets)*.8):]\n",
    "for i in range(len(ML_Models)):\n",
    "    testModel(ML_Models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 24, but received input with shape (32, 46)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 46), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[0;32m     21\u001b[0m curr_time \u001b[38;5;241m=\u001b[39m process_time_ns()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtestModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     23\u001b[0m timeTaken \u001b[38;5;241m=\u001b[39m process_time_ns \u001b[38;5;241m-\u001b[39m curr_time\n\u001b[0;32m     25\u001b[0m output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mModels[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeTaken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mtestModel\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      8\u001b[0m     y_test\u001b[38;5;241m.\u001b[39mappend(ATTACKS_ENUM[dict_7classes[k]]\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m TYPES[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m==\u001b[39mTYPES[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_CNN\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 10\u001b[0m     y_predict\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m==\u001b[39m TYPES[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF_RNN\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     12\u001b[0m     x_test\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(x_test, (x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, numFeatures))\n",
      "File \u001b[1;32mc:\\Users\\varte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\varte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 24, but received input with shape (32, 46)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 46), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# ===== Prediction time comparison ====\n",
    "# == Remember to check if correct X_columns\n",
    "from time import process_time_ns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# test_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')] \n",
    "folder = 'SavedModels/PerPacket/'\n",
    "Models = [\"ANN1.keras\", 'CNN1.pkl', 'DT.pkl', \"KNNC.pkl\", \"Lin-SVC.pkl\",\"LR1.pkl\",\"SGDC1.pkl\"] # also test RF from modelTooBig\n",
    "ModelTypes = [\"TF\", 'TF_CNN', 'SK_RF', \"SK_LR\", \"SK_LR\",\"SK_LR\",\"SK_LR\"]\n",
    "\n",
    "output = ''\n",
    "for i in range(len(Models)):\n",
    "    model_name = Models[i].split('.')\n",
    "    # print(len(model_name))\n",
    "    # print(model_name[0])\n",
    "    # print(model_name[1])\n",
    "    if model_name[1] == 'pkl':\n",
    "       model = Model(joblib.load(folder+Models[i]),model_name[0], TYPES[ModelTypes[i]])\n",
    "    elif model_name[1] == 'keras':\n",
    "        model = Model(load_model(folder+Models[i]),model_name[0], TYPES[ModelTypes[i]])\n",
    "    else:\n",
    "        print(f'Error occured at {i} model {Models[i]} type {ModelTypes[i]} : {model_name[0]} - {model.name[1]}')\n",
    "        raise Exception\n",
    "    curr_time = process_time_ns()\n",
    "    onlyPredict(model) \n",
    "    timeTaken = process_time_ns - curr_time\n",
    "\n",
    "    output += f\"Model {Models[i]} took {timeTaken} ns\"\n",
    "    toDisplay = RocCurveDisplay.from_estimator(...)\n",
    "    toDisplay.plot()\n",
    "    plt.savefig(fname=f'outputs/{model_name[0]}.png')\n",
    "    # plt.show()\n",
    "joblib.dump(output, f\"outputs/PerPacketTimes.txt\", protocol=1) \n",
    "\n",
    "\n",
    "\n",
    "toDisplay = RocCurveDisplay.from_estimator(...)\n",
    "toDisplay.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {\n",
    "#     # 'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "#     'loss': ['log_loss'],\n",
    "#     'penalty': ['l2', 'l1'],\n",
    "#     # 'penalty': ['l2'],\n",
    "#     'alpha': [1e-10, 1e-9, 1e-8, 1e-7],\n",
    "#     'max_iter': [1000],\n",
    "#     'tol': [1e-5]\n",
    "# }\n",
    "# grid_search = GridSearchCV(SGDClassifier(), param_grid, scoring='accuracy', n_jobs=-1)\n",
    "# print(f\"Last ran on grid, with {len(training_sets)} training sets on date: {datetime.datetime.now()}\")\n",
    "# for train_set in tqdm(training_sets):\n",
    "#     df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#     x_train = scaler.transform(df[X_columns])\n",
    "#     y_train = [ATTACKS_ENUM[dict_7classes[k]].value for k in df[Y_columns]]\n",
    "\n",
    "#     grid_search.fit(x_train, y_train)                   \n",
    "#     del df\n",
    "#     del x_train\n",
    "#     del y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# testModel(Model(best_model, \"SGDC FromGrid\", TYPES[\"SK_LR\"]))\n",
    "# # print(\"Best Model Accuracy:\", best_model.score(x_test, y_test))\n",
    "# # print(\"Best Model Classification Report:\\n\", classification_report(y_test, best_model.predict(X_test)))\n",
    "# # print(\"Best Model Confusion Matrix:\\n\", confusion_matrix(y_test, best_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====For debug ===\n",
    "# test_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')] \n",
    "#testModel(Model(load_model(\"SavedModels\\\\BestANN.keras\"),\"BestANN\", TYPES['TF']))\n",
    "# testModel(Model(joblib.load(\"SavedModels/BestRF.pkl\"),\"BestRF\", TYPES['SK_RF']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = getSimpleRFModel()\n",
    "# num_sample = 10\n",
    "# for train_set in tqdm(training_sets):\n",
    "#     df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#     df = df[0:num_sample]\n",
    "#     x_train = scaler.transform(df[X_columns])\n",
    "#     y_train = [ATTACKS_ENUM[dict_7classes[k]].value for k in df[Y_columns]]\n",
    "#     y_train_Cat = to_categorical(y_train, num_classes=8)\n",
    "\n",
    "    \n",
    "#     if model.type==TYPES[\"SK_LR\"]:\n",
    "#         model.model.fit(x_train, y_train)  \n",
    "    \n",
    "#     elif model.type==TYPES[\"SK_RF\"]:\n",
    "#         model.model.fit(x_train, y_train_Cat)  \n",
    "\n",
    "#     elif model.type == TYPES[\"TF\"]:\n",
    "#         model.model.fit(x=x_train, \n",
    "#                     y=y_train_Cat, \n",
    "#                     epochs=epochs, \n",
    "#                     verbose=verbose,\n",
    "#                     batch_size=batch_size)                   \n",
    "#     del df\n",
    "#     del x_train\n",
    "#     del y_train\n",
    "#     del y_train_Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tree Visualisation\n",
    "# from sklearn.tree import export_graphviz\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn import tree\n",
    "# rf = joblib.load(\"SavedModels/BestRF.pkl\")\n",
    "# for tree_in_rf in rf:\n",
    "#     fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "#     # export_graphviz(tree,\n",
    "#     #             feature_names=X_columns,\n",
    "#     #             class_names=ATTACKS,\n",
    "#     #             filled=True,\n",
    "#     #             rounded=True)\n",
    "#     tree.plot_tree(rf.estimators_[0],\n",
    "#                 feature_names=X_columns,\n",
    "#                 class_names=ATTACKS,\n",
    "#                filled = True)\n",
    "#     fig.savefig(f'rf_{i}.png')\n",
    "#     i+=1\n",
    "# for tree_in_rf in rf[:1]:\n",
    "#     export_graphviz(tree_in_rf,\n",
    "#                 feature_names=X_columns,\n",
    "#                 class_names=ATTACKS,\n",
    "#                 filled=True,\n",
    "#                 rounded=True,\n",
    "#                 out_file=\"tree.png\")\n",
    "#     os.system(\"-Tpng tree.dot -o tree.png\")\n",
    "# fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "# tree.plot_tree(model.model.estimators_[0])#,\n",
    "#             # feature_names=X_columns,\n",
    "#             # class_names=ATTACKS,\n",
    "#             # filled = True)\n",
    "# fig.savefig(f'rf_{num_sample}_samples.png')\n",
    "\n",
    "\n",
    "\n",
    "# # == Feature importance for RF and DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(ML_Models)):\n",
    "#     model = ML_Models[i]\n",
    "#     if model.type==TYPES[\"TF\"]:\n",
    "#         model.model.save(f\"SavedModels\\\\{model.name}.keras\",overwrite=True)\n",
    "#     elif model.type==TYPES[\"SK_RF\"] or model.type==TYPES[\"SK_LR\"]:\n",
    "#         joblib.dump(model.model, f\"SavedModels/{model.name}.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a seperate model to detect each attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose, epochs, batch_size = 1, 100, 512\n",
    "# activationFunction='relu'\n",
    "\n",
    "# def getSequentialModel():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(128, activation=activationFunction))\n",
    "#     model.add(Dense(64, activation=activationFunction))\n",
    "#     model.add(Dense(32, activation=activationFunction))\n",
    "#     model.add(Dense(16, activation=activationFunction))\n",
    "#     model.add(Dense(8, activation=activationFunction))\n",
    "#     model.add(Dense(4, activation=activationFunction))\n",
    "#     model.add(Dense(2, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "#                     optimizer=keras.optimizers.Adam(learning_rate=1e-3), \n",
    "#                     metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives()]\n",
    "#                     )\n",
    "#     return model\n",
    "\n",
    "# ML_Models = [\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel(),\n",
    "#             getSequentialModel()\n",
    "\n",
    "# ]\n",
    "# ML_Model_Names = ATTACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Last ran on {len(ML_Models)} models, with {len(training_sets)} training sets on date: {datetime.datetime.now()}\")\n",
    "# for train_set in tqdm(training_sets):\n",
    "#     df = pd.read_csv(DATASET_DIRECTORY + train_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#     x_train = scaler.transform(df[X_columns])\n",
    "\n",
    "#     for i in range(len(ML_Models)-1):\n",
    "#             y_train = to_categorical([ATTACKS_ENUM[dict_7classes[k]].value == ATTACKS_ENUM[ATTACKS[i]].value for k in df[Y_columns]], num_classes=2)\n",
    "#             model = ML_Models[i]\n",
    "#             model.fit(x=x_train, \n",
    "#                         y=y_train, \n",
    "#                         epochs=epochs, \n",
    "#                         verbose=verbose,\n",
    "#                         batch_size=batch_size)   \n",
    "#             del y_train             \n",
    "#     del df\n",
    "#     del x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def showResults8Models(test, pred, model_num):\n",
    "#     print(f\"===== {model_num} =====\")\n",
    "#     print(classification_report(test, pred, target_names=[\"Negative\", \"Positive\"]))\n",
    "#     accuracy = accuracy_score(test, pred)\n",
    "#     precision=precision_score(test, pred, average='weighted')\n",
    "#     f1Score=f1_score(test, pred, average='weighted') \n",
    "#     print(\"Accuracy  : {}\".format(accuracy))\n",
    "#     print(\"Precision : {}\".format(precision))\n",
    "#     print(\"f1Score : {}\".format(f1Score))\n",
    "#     cm=confusion_matrix(test, pred)\n",
    "#     print(cm) \n",
    "\n",
    "# print(f\"Last ran on {len(ML_Models)} models, with {len(test_sets)} testing sets on date: {datetime.datetime.now()}\")\n",
    "# for i in range(len(ML_Models)):\n",
    "#     model = ML_Models[i]\n",
    "#     y_test = []\n",
    "#     y_predict = []\n",
    "#     for test_set in tqdm(test_sets):\n",
    "#         df = pd.read_csv(DATASET_DIRECTORY + test_set, index_col=None, header=0, delimiter=',')[all_columns]\n",
    "#         x_test = scaler.transform(df[X_columns])\n",
    "#         for k in df[Y_columns]:\n",
    "#             y_test.append(ATTACKS_ENUM[dict_7classes[k]].value==ATTACKS[i])\n",
    "#         y_predict+= list(model.predict(x_test))\n",
    "\n",
    "#         del df\n",
    "#         del x_test\n",
    "\n",
    "#     myarr = np.array([ATTACKS_ENUM[dict_7classes[k]].value == ATTACKS_ENUM[ATTACKS[0]].value for k in ['DDoS-RSTFINFlood','DDoS-PSHACK_Flood','DDoS-SYN_Flood','DoS-SYN_Flood','DoS-TCP_Flood','Mirai-udpplain','Recon-OSScan','DNS_Spoofing','BrowserHijacking','Backdoor_Malware','DictionaryBruteForce']])\n",
    "#     print(myarr)\n",
    "#     print(to_categorical(myarr, num_classes=2))\n",
    "#     y_test=np.array(y_test)\n",
    "#     print(y_test[0:10])\n",
    "#     y_test = to_categorical(y_test, num_classes=2)\n",
    "#     print(y_test[0:10])\n",
    "#     print(\"=========\")\n",
    "#     for i in range(10):\n",
    "#         print(f\"{i}: {y_predict[i]} actual {y_test[i]}\")    \n",
    "\n",
    "#     test = np.argmax(y_test, axis=1)\n",
    "#     predict = np.argmax(y_predict, axis=1)\n",
    "#     showResults8Models(test, predict, i)\n",
    "\n",
    "#     del test\n",
    "#     del predict\n",
    "#     del y_test\n",
    "#     del y_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
